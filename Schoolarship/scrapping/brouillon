
# # import pandas as pd
# # import numpy as np

# # from datetime import datetime
# # from sqlalchemy import create_engine
# # import psycopg2

# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux noms des champs dans la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Vérification des types de données pour la colonne 'date_limite'
# # #df['date_limite'] = pd.to_datetime(df['date_limite'], errors='coerce')

# # # Insertion des données dans la base de données PostgreSQL
# # #df.to_sql('opportunities_etudes', engine, if_exists='append', index=False)


# # df['date_limite'] = df['date_limite'].replace('Date limite non spécifiée', np.nan)

# # # Convertir 'Date Limite' en format datetime
# # def parse_date(date_str):
# #     if pd.isnull(date_str):
# #         return pd.NaT
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt)
# #         except ValueError:
# #             continue
# #     return pd.NaT


# # df['date_limite'] = df['date_limite'].apply(parse_date)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # print(df)

# # # creating connexion
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     password='scholarship',
# #     user='opportunities',
# #     database='study_opportunities'
# # )


# # cursor_postgres = conn.cursor()

# # create_postgres_query_file = open("./databases/postgres_create_tables.sql")
# # create_postgres_query = create_postgres_query_file.read()

# # cursor_postgres.execute(create_postgres_query)

# # # commiting 
# # conn.commit()

# # # Enregistrer les données dans PostgreSQL
# # # db_url = 'postgresql://opportunities:scholarship@localhost:5432/study_opportunities'
# # #engine = create_engine(db_url)
# # # engine creating to avoid warning
# # engine = create_engine("postgresql+psycopg2://", creator=lambda:conn)

# # a = pd.read_sql("SELECT * FROM opportunities_etudes;", con=engine)

# # print(a)

# # # on crrer une liste de donnees
# # data = list( df.itertuples(index=None, name=None))

# # # apres avoir creer le fichier contenant la requete d'insertion des donnees dans la table universite dans le dossier sql, on charge le fichier sql pour creer la table dans mysql
# # merge_postgres_query_file = open('./databases/postgres_upsert.sql')
# # merge_postgres_query = merge_postgres_query_file.read() # lecture 

# # pd.read_sql("SELECT * FROM opportunities_etudes;", con=engine)


# # import pandas as pd
# # import numpy as np
# # from datetime import datetime
# # import psycopg2

# # # Charger les données depuis le fichier CSV
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux champs de la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Remplacer les valeurs "Date limite non spécifiée" par NaN
# # df['date_limite'] = df['date_limite'].replace('Date limite non spécifiée', np.nan)

# # # Fonction pour convertir les dates au format datetime
# # def parse_date(date_str):
# #     if pd.isnull(date_str):
# #         return pd.NaT
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt)
# #         except ValueError:
# #             continue
# #     return pd.NaT

# # # Appliquer la fonction de conversion sur la colonne 'date_limite'
# # df['date_limite'] = df['date_limite'].apply(parse_date)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # # Connexion à la base de données PostgreSQL
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     user='opportunities',
# #     password='scholarship',
# #     database='study_opportunities'
# # )

# # cursor_postgres = conn.cursor()

# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as merge_postgres_query_file:
# #     merge_postgres_query = merge_postgres_query_file.read()

# # # Itérer sur chaque ligne du DataFrame et insérer les données
# # for row in df.itertuples(index=False, name=None):
# #     cursor_postgres.execute(merge_postgres_query, row)

# # # Valider les transactions
# # conn.commit()

# # # Vérification : Exécuter une requête pour voir si les données sont bien insérées
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()
# # for row in rows:
# #     print(row)

# # # Fermer la connexion
# # cursor_postgres.close()
# # conn.close()


# # import pandas as pd
# # import numpy as np
# # from datetime import datetime
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux champs de la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Conserver les URLs et valeurs spéciales dans la colonne 'date_limite'
# # # On ne remplace pas les valeurs spéciales ici

# # # Fonction pour convertir les dates au format datetime, si elles sont valides
# # def parse_date(date_str):
# #     if pd.isnull(date_str) or (isinstance(date_str, str) and date_str.startswith('http')):
# #         return date_str  # Conserver les URLs et NaN
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')  # Format pour la base de données
# #         except ValueError:
# #             continue
# #     return date_str

# # # Appliquer la fonction de conversion sur la colonne 'date_limite'
# # df['date_limite'] = df['date_limite'].apply(parse_date)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # # Connexion à la base de données PostgreSQL
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     user='opportunities',
# #     password='scholarship',
# #     database='study_opportunities'
# # )

# # cursor_postgres = conn.cursor()

# # # Charger et exécuter la requête de création de table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()

# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en liste de tuples
# # data = list(df.itertuples(index=False, name=None))

# # # Insérer les données dans la base de données
# # try:
# #     execute_values(cursor_postgres, merge_postgres_query, data)
# #     conn.commit()
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()

# # # Vérification : Exécuter une requête pour voir toutes les données
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()
# # for row in rows:
# #     print(row)

# # # Fermer la connexion
# # cursor_postgres.close()
# # conn.close()


# # import pandas as pd
# # import numpy as np
# # from datetime import datetime
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux champs de la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Conserver les URLs et les chaînes non valides dans une nouvelle colonne
# # df['date_limite_raw'] = df['date_limite']

# # # Fonction pour convertir les dates au format datetime
# # def parse_date(date_str):
# #     if pd.isnull(date_str):
# #         return pd.NaT  # Retourner NaT pour les valeurs NaN
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')  # Format pour la base de données
# #         except ValueError:
# #             continue
# #     return np.nan  # Retourner NaN pour les chaînes non valides

# # # Convertir les dates et conserver les chaînes non valides ou les URLs
# # df['date_limite'] = df['date_limite'].apply(lambda x: parse_date(x) if pd.notna(x) and not x.startswith('http') and x != 'Date limite non spécifiée' else np.nan)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # # Connexion à la base de données PostgreSQL
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     user='opportunities',
# #     password='scholarship',
# #     database='study_opportunities'
# # )

# # cursor_postgres = conn.cursor()

# # # Charger et exécuter la requête de création de table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()

# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en liste de tuples
# # data = list(df.itertuples(index=False, name=None))

# # # Insérer les données dans la base de données
# # try:
# #     execute_values(cursor_postgres, merge_postgres_query, data)
# #     conn.commit()
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()

# # # Vérification : Exécuter une requête pour voir toutes les données
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()
# # for row in rows:
# #     print(row)

# # # Fermer la connexion
# # cursor_postgres.close()
# # conn.close()


# # import pandas as pd
# # import numpy as np
# # from datetime import datetime
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux champs de la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Conserver les URLs et les chaînes non valides dans une nouvelle colonne
# # df['date_limite_raw'] = df['date_limite']

# # # Fonction pour convertir les dates au format datetime
# # def parse_date(date_str):
# #     if pd.isnull(date_str):
# #         return pd.NaT  # Retourner NaT pour les valeurs NaN
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')  # Format pour la base de données
# #         except ValueError:
# #             continue
# #     return np.nan  # Retourner NaN pour les chaînes non valides

# # # Convertir les dates et conserver les chaînes non valides ou les URLs
# # df['date_limite'] = df['date_limite'].apply(lambda x: parse_date(x) if pd.notna(x) and not x.startswith('http') and x != 'Date limite non spécifiée' else np.nan)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # # Afficher les données pour déboguer
# # print(df.head())
# # print(df.columns)

# # # Connexion à la base de données PostgreSQL
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     user='opportunities',
# #     password='scholarship',
# #     database='study_opportunities'
# # )

# # cursor_postgres = conn.cursor()

# # # Charger et exécuter la requête de création de table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()

# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en liste de tuples
# # data = list(df.itertuples(index=False, name=None))

# # # Afficher les données pour déboguer
# # print("Data to insert:")
# # for row in data[:15]:  # Afficher seulement les 5 premières lignes pour éviter l'encombrement
# #     print(row)

# # # Insérer les données dans la base de données
# # try:
# #     execute_values(cursor_postgres, merge_postgres_query, data)
# #     conn.commit()
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()

# # # Vérification : Exécuter une requête pour voir toutes les données
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()
# # for row in rows:
# #     print(row)

# # # Fermer la connexion
# # cursor_postgres.close()
# # conn.close()


# # import pandas as pd
# # import numpy as np
# # from datetime import datetime
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes pour correspondre aux champs de la table SQL (en minuscules)
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Fonction pour convertir les dates au format datetime
# # def parse_date(date_str):
# #     if pd.isnull(date_str):
# #         return pd.NaT  # Retourner NaT pour les valeurs NaN
# #     for fmt in ('%d-%m-%Y', '%d/%m/%Y', '%d %B %Y'):
# #         try:
# #             return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')  # Format pour la base de données
# #         except ValueError:
# #             continue
# #     return np.nan  # Retourner NaN pour les chaînes non valides

# # # Convertir les dates et conserver les chaînes non valides ou les URLs
# # df['date_limite'] = df['date_limite'].apply(lambda x: parse_date(x) if pd.notna(x) and not x.startswith('http') and x != 'Date limite non spécifiée' else np.nan)

# # # Supprimer les doublons
# # df.drop_duplicates(inplace=True)

# # # Afficher les données pour déboguer
# # print(df.head())
# # print(df.columns)

# # # Connexion à la base de données PostgreSQL
# # conn = psycopg2.connect(
# #     host='localhost',
# #     port=5432,
# #     user='opportunities',
# #     password='scholarship',
# #     database='study_opportunities'
# # )

# # cursor_postgres = conn.cursor()

# # # Charger et exécuter la requête de création de table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()

# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en liste de tuples
# # data = list(df.itertuples(index=False, name=None))

# # # Afficher les données pour déboguer
# # print("Data to insert:")
# # for row in data[:5]:  # Afficher seulement les 5 premières lignes pour éviter l'encombrement
# #     print(row)

# # # Insérer les données dans la base de données
# # try:
# #     execute_values(cursor_postgres, merge_postgres_query, data)
# #     conn.commit()
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()

# # # Vérification : Exécuter une requête pour voir toutes les données
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()
# # for row in rows:
# #     print(row)

# # # Fermer la connexion
# # cursor_postgres.close()
# # conn.close()


# # # Importer les bibliothèques nécessaires
# # import pandas as pd
# # import numpy as np
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# # # Cette étape facilite l'insertion des données dans la base de données
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Aucun traitement particulier n'est fait sur la colonne 'date_limite' ; elle est conservée telle quelle
# # # Les valeurs invalides, telles que les chaînes de caractères non valides ou les URLs, sont conservées

# # # Supprimer les doublons du DataFrame pour éviter l'insertion de lignes identiques dans la base de données
# # df.drop_duplicates(inplace=True)

# # # Afficher les premières lignes du DataFrame et les noms des colonnes pour vérifier les données
# # print(df.head())
# # print(df.columns)
# # print(df.dtypes)


# # # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# # conn = psycopg2.connect(
# #     host='localhost',      # Adresse du serveur de base de données
# #     port=5432,             # Port sur lequel le serveur écoute
# #     user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
# #     password='scholarship',# Mot de passe de l'utilisateur
# #     database='study_opportunities'  # Nom de la base de données
# # )

# # # Créer un curseur pour exécuter des requêtes SQL
# # cursor_postgres = conn.cursor()

# # # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # # Essayer d'exécuter la requête de création de table
# # # Si la table existe déjà, l'opération ne posera pas de problème
# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Charger et lire la requête SQL pour insérer ou mettre à jour les données depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en une liste de tuples, où chaque tuple représente une ligne de données
# # data = list(df.itertuples(index=None, name=None))

# # # Afficher les premières lignes de données pour vérifier avant l'insertion
# # print("Data to insert:")
# # for row in data[:5]:  # Afficher seulement les 5 premières lignes pour éviter l'encombrement
# #     print(row)

# # # Essayer d'exécuter la requête d'insertion ou de mise à jour des données dans la base de données
# # try:
# #     #execute_values(cursor_postgres, merge_postgres_query, data)
# #     cursor_postgres.executemany(merge_postgres_query, data)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Vérifier les données insérées en exécutant une requête SELECT pour récupérer toutes les données de la table
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()  # Récupérer toutes les lignes de résultats
# # for row in rows:
# #     print(row)  # Afficher chaque ligne de résultat

# # # Fermer le curseur et la connexion à la base de données
# # cursor_postgres.close()
# # conn.close()


# # # Importer les bibliothèques nécessaires
# # import pandas as pd
# # import numpy as np
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# # df = pd.read_csv("opportunities_etudes.csv")

# # # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Fonction pour formater la colonne 'conditions'
# # def format_conditions(conditions):
# #     if isinstance(conditions, str) and conditions.startswith('[') and conditions.endswith(']'):
# #         # Supprimer les crochets et les espaces superflus
# #         links = conditions.strip('[]').replace("'", "").split(',')
# #         # Joindre les liens avec une délimitation appropriée
# #         return ', '.join(link.strip() for link in links)
# #     return conditions

# # # Appliquer le formatage à la colonne 'conditions'
# # df['conditions'] = df['conditions'].apply(format_conditions)

# # # Supprimer les doublons du DataFrame pour éviter l'insertion de lignes identiques dans la base de données
# # df.drop_duplicates(inplace=True)

# # # Afficher les premières lignes du DataFrame et les noms des colonnes pour vérifier les données
# # print(df.head())
# # print(df.columns)
# # print(df.dtypes)

# # # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# # conn = psycopg2.connect(
# #     host='localhost',      # Adresse du serveur de base de données
# #     port=5432,             # Port sur lequel le serveur écoute
# #     user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
# #     password='scholarship',# Mot de passe de l'utilisateur
# #     database='study_opportunities'  # Nom de la base de données
# # )

# # # Créer un curseur pour exécuter des requêtes SQL
# # cursor_postgres = conn.cursor()

# # # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # # Essayer d'exécuter la requête de création de table
# # # Si la table existe déjà, l'opération ne posera pas de problème
# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Charger et lire la requête SQL pour insérer ou mettre à jour les données depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en une liste de tuples, où chaque tuple représente une ligne de données
# # data = list(df.itertuples(index=None, name=None))

# # # Afficher les premières lignes de données pour vérifier avant l'insertion
# # print("Data to insert:")
# # for row in data[:5]:  # Afficher seulement les 5 premières lignes pour éviter l'encombrement
# #     print(row)

# # # Essayer d'exécuter la requête d'insertion ou de mise à jour des données dans la base de données
# # try:
# #     cursor_postgres.executemany(merge_postgres_query, data)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Vérifier les données insérées en exécutant une requête SELECT pour récupérer toutes les données de la table
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()  # Récupérer toutes les lignes de résultats
# # for row in rows:
# #     print(row)  # Afficher chaque ligne de résultat

# # # Fermer le curseur et la connexion à la base de données
# # cursor_postgres.close()
# # conn.close()


# # # Importer les bibliothèques nécessaires
# # import pandas as pd
# # import numpy as np
# # import psycopg2
# # from psycopg2.extras import execute_values

# # # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# # df = pd.read_csv("stages.csv")

# # # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # # Fonction pour formater la colonne 'conditions'
# # # def format_conditions(conditions):
# # #     if isinstance(conditions, str):
# # #         # Vérifier si les conditions sont au format liste entre crochets
# # #         if conditions.startswith('[') and conditions.endswith(']'):
# # #             # Supprimer les crochets et les guillemets simples, séparer par des virgules
# # #             links = conditions.strip('[]').replace("'", "").split(',')
# # #             # Nettoyer chaque lien en supprimant les espaces superflus
# # #             return ', '.join(link.strip() for link in links)
# # #     return conditions

# # # # Appliquer le formatage à la colonne 'conditions'
# # # df['conditions'] = df['conditions'].apply(format_conditions)

# # # Supprimer les doublons du DataFrame pour éviter l'insertion de lignes identiques dans la base de données
# # df.drop_duplicates(inplace=True)


# # # Afficher les premières lignes du DataFrame et les noms des colonnes pour vérifier les données
# # print(df.head())
# # df = df.drop(columns=['Unnamed: 0'])

# # print(df.columns)
# # print(df.dtypes)

# # # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# # conn = psycopg2.connect(
# #     host='localhost',      # Adresse du serveur de base de données
# #     port=5432,             # Port sur lequel le serveur écoute
# #     user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
# #     password='scholarship',# Mot de passe de l'utilisateur
# #     database='study_opportunities'  # Nom de la base de données
# # )

# # # Créer un curseur pour exécuter des requêtes SQL
# # cursor_postgres = conn.cursor()

# # # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # # Essayer d'exécuter la requête de création de table
# # # Si la table existe déjà, l'opération ne posera pas de problème
# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Charger et lire la requête SQL pour insérer ou mettre à jour les données depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     merge_postgres_query = file.read()

# # # Convertir le DataFrame en une liste de tuples, où chaque tuple représente une ligne de données
# # data = list(df.itertuples(index=None, name=None))

# # # Afficher les premières lignes de données pour vérifier avant l'insertion
# # print("Data to insert:")
# # # for row in data[:5]:  # Afficher seulement les 5 premières lignes pour éviter l'encombrement
# # #     print(row)

# # data = data[:5]

# # # Essayer d'exécuter la requête d'insertion ou de mise à jour des données dans la base de données
# # try:
# #     #execute_values(cursor_postgres, merge_postgres_query, data)
# #     print(df.columns)
# #     print(len(df.columns))
# #     cursor_postgres.executemany(merge_postgres_query, data)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("Les données ont été insérées ou mises à jour avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Vérifier les données insérées en exécutant une requête SELECT pour récupérer toutes les données de la table
# # cursor_postgres.execute("SELECT * FROM opportunities_etudes;")
# # rows = cursor_postgres.fetchall()  # Récupérer toutes les lignes de résultats
# # for row in rows:
# #     print(row)  # Afficher chaque ligne de résultat

# # # Fermer le curseur et la connexion à la base de données
# # cursor_postgres.close()
# # conn.close()


# # # Importer les bibliothèques nécessaires
# # import pandas as pd
# # import psycopg2
# # from psycopg2 import sql

# # # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# # df = pd.read_csv("stages.csv")

# # # Supprimer la colonne "Unnamed: 0" si elle existe
# # if 'Unnamed: 0' in df.columns:
# #     df = df.drop(columns=['Unnamed: 0'])

# # # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Vérifier le DataFrame après les transformations
# # print(df.head())  # Afficher les premières lignes pour vérifier
# # print(df.columns)  # Vérifier les colonnes après renommage
# # print(df.dtypes)   # Vérifier les types de données des colonnes

# # # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# # try:
# #     conn = psycopg2.connect(
# #         host='localhost',      # Adresse du serveur de base de données
# #         port=5432,             # Port sur lequel le serveur écoute
# #         user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
# #         password='scholarship',# Mot de passe de l'utilisateur
# #         database='study_opportunities'  # Nom de la base de données
# #     )
# #     print("Connexion réussie à la base de données.")
# # except Exception as e:
# #     print(f"Erreur de connexion : {e}")

# # # Créer un curseur pour exécuter des requêtes SQL
# # cursor_postgres = conn.cursor()

# # # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # # Essayer d'exécuter la requête de création de table
# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Préparer la requête d'insertion SQL
# # insert_query = sql.SQL("""
# #     INSERT INTO opportunities_etudes (
# #         pays, titre, type, description, niveau, financement, date_limite,
# #         conditions, nombre_de_bourses, domaine_concerne, duree_d_etude, pays_eligibles
# #     ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
# #     ON CONFLICT (pays, titre)
# #     DO UPDATE SET
# #         type = EXCLUDED.type,
# #         description = EXCLUDED.description,
# #         niveau = EXCLUDED.niveau,
# #         financement = EXCLUDED.financement,
# #         date_limite = EXCLUDED.date_limite,
# #         conditions = EXCLUDED.conditions,
# #         nombre_de_bourses = EXCLUDED.nombre_de_bourses,
# #         domaine_concerne = EXCLUDED.domaine_concerne,
# #         duree_d_etude = EXCLUDED.duree_d_etude,
# #         pays_eligibles = EXCLUDED.pays_eligibles;

# # """)

# # # Convertir le DataFrame en une liste de tuples pour l'insertion
# # data = list(df.itertuples(index=False, name=None))

# # # Insérer les données dans la table PostgreSQL
# # try:
# #     cursor_postgres.executemany(insert_query, data)  # Utiliser executemany pour insérer plusieurs lignes à la fois
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("Les données ont été insérées avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Fermer le curseur et la connexion à la base de données
# # cursor_postgres.close()
# # conn.close()


# # # Importer les bibliothèques nécessaires
# # import pandas as pd
# # import psycopg2
# # from psycopg2 import sql

# # # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# # df = pd.read_csv("formations.csv")

# # # Supprimer la colonne "Unnamed: 0" si elle existe
# # if 'Unnamed: 0' in df.columns:
# #     df = df.drop(columns=['Unnamed: 0'])

# # # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# # df.rename(columns={
# #     'Pays': 'pays',
# #     'Titre': 'titre',
# #     'Type': 'type',
# #     'Description': 'description',
# #     'Niveau': 'niveau',
# #     'Financement': 'financement',
# #     'Date Limite': 'date_limite',
# #     'Conditions': 'conditions',
# #     'Nombre de bourses': 'nombre_de_bourses',
# #     'Domaine Conserné': 'domaine_concerne',
# #     'Durée d\'étude': 'duree_d_etude',
# #     'Pays éligibles': 'pays_eligibles'
# # }, inplace=True)

# # # Vérifier le DataFrame après les transformations
# # print(df.head())  # Afficher les premières lignes pour vérifier
# # print(df.columns)  # Vérifier les colonnes après renommage
# # print(df.dtypes)   # Vérifier les types de données des colonnes

# # # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# # try:
# #     conn = psycopg2.connect(
# #         host='localhost',      # Adresse du serveur de base de données
# #         port=5432,             # Port sur lequel le serveur écoute
# #         user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
# #         password='scholarship',# Mot de passe de l'utilisateur
# #         database='study_opportunities'  # Nom de la base de données
# #     )
# #     print("Connexion réussie à la base de données.")
# # except Exception as e:
# #     print(f"Erreur de connexion : {e}")

# # # Créer un curseur pour exécuter des requêtes SQL
# # cursor_postgres = conn.cursor()

# # # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# # with open('./databases/postgres_create_tables.sql', 'r') as file:
# #     create_table_query = file.read()

# # # Essayer d'exécuter la requête de création de table
# # try:
# #     cursor_postgres.execute(create_table_query)
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("La table a été créée ou existe déjà.")
# # except Exception as e:
# #     print(f"Erreur lors de la création de la table: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur


# # # Charger la requête d'insertion depuis le fichier SQL
# # with open('./databases/postgres_upsert.sql', 'r') as file:
# #     insert_query = file.read()


# # # # Préparer la requête d'insertion SQL
# # # insert_query = sql.SQL("""
# # #     INSERT INTO opportunities_etudes (
# # #         pays, titre, type, description, niveau, financement, date_limite,
# # #         conditions, nombre_de_bourses, domaine_concerne, duree_d_etude, pays_eligibles
# # #     ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
# # #     ON CONFLICT (pays, titre)
# # #     DO UPDATE SET
# # #         type = EXCLUDED.type,
# # #         description = EXCLUDED.description,
# # #         niveau = EXCLUDED.niveau,
# # #         financement = EXCLUDED.financement,
# # #         date_limite = EXCLUDED.date_limite,
# # #         conditions = EXCLUDED.conditions,
# # #         nombre_de_bourses = EXCLUDED.nombre_de_bourses,
# # #         domaine_concerne = EXCLUDED.domaine_concerne,
# # #         duree_d_etude = EXCLUDED.duree_d_etude,
# # #         pays_eligibles = EXCLUDED.pays_eligibles;

# # # """)

# # # Convertir le DataFrame en une liste de tuples pour l'insertion
# # data = list(df.itertuples(index=False, name=None))

# # # Insérer les données dans la table PostgreSQL
# # try:
# #     cursor_postgres.executemany(insert_query, data)  # Utiliser executemany pour insérer plusieurs lignes à la fois
# #     conn.commit()  # Valider les modifications apportées à la base de données
# #     print("Les données ont été insérées avec succès.")
# # except Exception as e:
# #     print(f"Erreur lors de l'insertion des données: {e}")
# #     conn.rollback()  # Annuler les modifications en cas d'erreur

# # # Fermer le curseur et la connexion à la base de données
# # cursor_postgres.close()
# # conn.close()


# # Importer les bibliothèques nécessaires
# import pandas as pd
# import psycopg2
# from psycopg2 import sql
# from sqlalchemy import create_engine


# # Charger les données depuis le fichier CSV dans un DataFrame Pandas
# df = pd.read_csv("opportunities_etudes.csv")

# # Supprimer la colonne "Unnamed: 0" si elle existe
# if 'Unnamed: 0' in df.columns:
#     df = df.drop(columns=['Unnamed: 0'])

# # Renommer les colonnes du DataFrame pour qu'elles correspondent aux champs de la table SQL
# df.rename(columns={
#     'Pays': 'pays',
#     'Titre': 'titre',
#     'Type': 'type',
#     'Description': 'description',
#     'Niveau': 'niveau',
#     'Financement': 'financement',
#     'Date Limite': 'date_limite',
#     'Conditions': 'conditions',
#     'Nombre de bourses': 'nombre_de_bourses',
#     'Domaine Conserné': 'domaine_concerne',
#     'Durée d\'étude': 'duree_d_etude',
#     'Pays éligibles': 'pays_eligibles'
# }, inplace=True)

# # Vérifier le DataFrame après les transformations
# print(df.head())  # Afficher les premières lignes pour vérifier
# print(df.columns)  # Vérifier les colonnes après renommage
# print(df.dtypes)   # Vérifier les types de données des colonnes

# # Connexion à la base de données PostgreSQL avec les paramètres spécifiés
# try:
#     conn = psycopg2.connect(
#         host='localhost',      # Adresse du serveur de base de données
#         port=5432,             # Port sur lequel le serveur écoute
#         user='opportunities',  # Nom d'utilisateur pour se connecter à la base de données
#         password='scholarship',# Mot de passe de l'utilisateur
#         database='study_opportunities'  # Nom de la base de données
#     )
#     print("Connexion réussie à la base de données.")
# except Exception as e:
#     print(f"Erreur de connexion : {e}")

# # Créer un curseur pour exécuter des requêtes SQL
# cursor_postgres = conn.cursor()

# # Charger et lire la requête SQL pour créer la table depuis le fichier SQL
# with open('./databases/postgres_create_tables.sql', 'r') as file:
#     create_table_query = file.read()

# # Essayer d'exécuter la requête de création de table
# try:
#     cursor_postgres.execute(create_table_query)
#     conn.commit()  # Valider les modifications apportées à la base de données
#     print("La table a été créée ou existe déjà.")
# except Exception as e:
#     print(f"Erreur lors de la création de la table: {e}")
#     conn.rollback()  # Annuler les modifications en cas d'erreur

# # Préparer la requête d'insertion SQL
# insert_query = sql.SQL("""
#     INSERT INTO opportunities_etudes (
#         pays, titre, type, description, niveau, financement, date_limite,
#         conditions, nombre_de_bourses, domaine_concerne, duree_d_etude, pays_eligibles
#     ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#     ON CONFLICT (pays, titre)
#     DO UPDATE SET
#         type = EXCLUDED.type,
#         description = EXCLUDED.description,
#         niveau = EXCLUDED.niveau,
#         financement = EXCLUDED.financement,
#         date_limite = EXCLUDED.date_limite,
#         conditions = EXCLUDED.conditions,
#         nombre_de_bourses = EXCLUDED.nombre_de_bourses,
#         domaine_concerne = EXCLUDED.domaine_concerne,
#         duree_d_etude = EXCLUDED.duree_d_etude,
#         pays_eligibles = EXCLUDED.pays_eligibles;

# """)

# # Convertir le DataFrame en une liste de tuples pour l'insertion
# data = list(df.itertuples(index=False, name=None))

# # Insérer les données dans la table PostgreSQL
# try:
#     cursor_postgres.executemany(insert_query, data)  # Utiliser executemany pour insérer plusieurs lignes à la fois
#     conn.commit()  # Valider les modifications apportées à la base de données
#     print("Les données ont été insérées avec succès.")
# except Exception as e:
#     print(f"Erreur lors de l'insertion des données: {e}")
#     conn.rollback()  # Annuler les modifications en cas d'erreur


# # Utiliser SQLAlchemy pour lire les données si nécessaire
# engine = create_engine('postgresql+psycopg2://opportunities:scholarship@localhost:5432/study_opportunities')
# try:
#     df_read = pd.read_sql("SELECT * FROM opportunities_etudes;", con=engine)
#     print(df_read.head())
# except Exception as e:
#     print(f"Erreur lors de la lecture des données : {e}")

# # Fermer le curseur et la connexion à la base de données
# cursor_postgres.close()
# conn.close()


Main

###############################Execution  unique du script d'installation de chrome et chrome-driver ###############

# # Chemin vers le fichier de configuration JSON
# CONFIG_FILE = 'config.json'

# def load_config():
#     """Charger la configuration depuis le fichier JSON"""
#     if not os.path.exists(CONFIG_FILE):
#         # Si le fichier n'existe pas, créer une configuration par défaut
#         return {"setup_done": False}
#     # Charger la configuration depuis le fichier JSON
#     with open(CONFIG_FILE, 'r') as f:
#         return json.load(f)

# def save_config(config):
#     """Enregistrer la configuration dans le fichier JSON"""
#     with open(CONFIG_FILE, 'w') as f:
#         json.dump(config, f, indent=4)  # Sauvegarder la configuration formatée

# def setup_once():
#     """Exécuter l'installation une seule fois en utilisant un fichier de configuration JSON"""
#     config = load_config()  # Charger la configuration actuelle
#     if not config.get("setup_done", False):
#         # Si l'installation n'a pas été faite, l'exécuter
#         install_python_dependencies()
#         install_chrome_and_chromedriver()
#         # Mettre à jour la configuration pour indiquer que le setup est fait
#         config["setup_done"] = True
#         save_config(config)  # Sauvegarder la configuration mise à jour

############################### Fin d'Execution  unique du script d'installation de chrome et chrome-driver ###############


@app.route('/')

@app.route('/run_scraping')
def run_scraping():
    result = scrape_and_store_data()
    return result


# @app.route('/scrape_campusfaso')
# def scrape_campusfaso():
#     df_bourses_campusfaso = scraper_bourses_campusfaso()
    
#     # Vérifier que le DataFrame contient des données
#     if not df_bourses_campusfaso.empty:
#         # Sauvegarder le DataFrame dans un fichier CSV
#         df_bourses_campusfaso.to_csv("bourses_campusfaso.csv", index=False)
#         return "Scraping de Campus Faso terminé et fichier CSV créé"
#     else:
#         return "Aucune donnée trouvée lors du scraping"
    


# @app.route('/scrape_opportunities_etudes')
# def scrape_opportunities_etudes():

#     # bourses sur campusfaso
#     df_bourses_campusfaso = scraper_bourses_campusfaso()

#     # Sauvegarder le DataFrame dans un fichier CSV
#     df_bourses_campusfaso.to_csv("bourses_campusfaso.csv", index=False)

#     ############## bourses, stages & formations sur greatyop ##

#     url_bourses = 'https://greatyop.com/category/bourses/'
#     # Appeler la fonction et stocker le nombre total de pages dans la variable 'total_pages_bourses'
#     total_pages_bourses = get_total_pages(url_bourses)

#     url_stages = 'https://greatyop.com/category/stages-emplois/'
#     # Appeler la fonction et stocker le nombre total de pages dans la variable 'total_pages_stages'
#     total_pages_stages = get_total_pages(url_stages)

#     url_formations = 'https://greatyop.com/category/formations/'
#     # Appeler la fonction et stocker le nombre total de pages dans la variable 'total_pages_formations'
#     total_pages_formations = get_total_pages(url_formations)


#     urls_pages_bourses = generate_page_urls(url_bourses, total_pages_bourses)

#     urls_pages_stages = generate_page_urls(url_stages, total_pages_stages)

#     urls_pages_formations = generate_page_urls(url_formations, total_pages_formations)


#     liens_bourses = extract_links_from_pages(urls_pages_bourses)

#     liens_stages = extract_links_from_pages(urls_pages_stages)

#     liens_formations = extract_links_from_pages(urls_pages_formations)


#     bourses = extract_bourse_info_from_urls(liens_bourses)
#     df_bourses = pd.DataFrame(bourses)
#     df_bourses.to_csv('bourses.csv')

#     stages = extract_bourse_info_from_urls(liens_stages)
#     df_stages = pd.DataFrame(stages)
#     df_stages['Type'] = 'Stage'
#     df_stages.to_csv('stages.csv')

#     formations = extract_bourse_info_from_urls(liens_formations)
#     df_formations = pd.DataFrame(formations)
#     df_formations['Type'] = 'Formation'
#     df_formations.to_csv('formations.csv')

#     df = pd.concat([df_bourses_campusfaso,df_bourses, df_stages, df_formations], ignore_index=True)
#     df.to_csv('opportunities_etudes.csv')



# @app.route('/scrape_opportunities_etudes')
# def scrape_opportunities_etudes():
#     try:
#         # Scraping des bourses sur CampusFaso
#         df_bourses_campusfaso = scraper_bourses_campusfaso()
        
#         # Vérifier si le DataFrame n'est pas vide
#         if not df_bourses_campusfaso.empty:
#             df_bourses_campusfaso.to_csv("bourses_campusfaso.csv", index=False)
#         else:
#             print("Le DataFrame df_bourses_campusfaso est vide.")
        
#         # Scraping des bourses, stages et formations sur GreatYop
#         url_bourses = 'https://greatyop.com/category/bourses/'
#         total_pages_bourses = get_total_pages(url_bourses)

#         url_stages = 'https://greatyop.com/category/stages-emplois/'
#         total_pages_stages = get_total_pages(url_stages)

#         url_formations = 'https://greatyop.com/category/formations/'
#         total_pages_formations = get_total_pages(url_formations)

#         urls_pages_bourses = generate_page_urls(url_bourses, total_pages_bourses)
#         urls_pages_stages = generate_page_urls(url_stages, total_pages_stages)
#         urls_pages_formations = generate_page_urls(url_formations, total_pages_formations)

#         liens_bourses = extract_links_from_pages(urls_pages_bourses)
#         liens_stages = extract_links_from_pages(urls_pages_stages)
#         liens_formations = extract_links_from_pages(urls_pages_formations)

#         bourses = extract_bourse_info_from_urls(liens_bourses)
#         df_bourses = pd.DataFrame(bourses)
#         if not df_bourses.empty:
#             df_bourses.to_csv('bourses.csv', index=False)
#         else:
#             print("Le DataFrame df_bourses est vide.")
        
#         stages = extract_bourse_info_from_urls(liens_stages)
#         df_stages = pd.DataFrame(stages)
#         if not df_stages.empty:
#             df_stages['Type'] = 'Stage'
#             df_stages.to_csv('stages.csv', index=False)
#         else:
#             print("Le DataFrame df_stages est vide.")
        
#         formations = extract_bourse_info_from_urls(liens_formations)
#         df_formations = pd.DataFrame(formations)
#         if not df_formations.empty:
#             df_formations['Type'] = 'Formation'
#             df_formations.to_csv('formations.csv', index=False)
#         else:
#             print("Le DataFrame df_formations est vide.")
        
#         # Concaténer tous les DataFrames et sauvegarder en CSV
#         if not df_bourses_campusfaso.empty and not df_bourses.empty and not df_stages.empty and not df_formations.empty:
#             df = pd.concat([df_bourses_campusfaso, df_bourses, df_stages, df_formations], ignore_index=True)
#             df.to_csv('opportunities_etudes.csv', index=False)
#         else:
#             print("Un ou plusieurs DataFrames sont vides. Aucun fichier CSV 'opportunities_etudes.csv' n'a été créé.")
        
#         return "Scraping terminé et fichiers CSV créés"

#     except Exception as e:
#         # Afficher les erreurs éventuelles
#         print(f"Erreur lors du scraping ou de la sauvegarde des fichiers CSV: {str(e)}")
#         return f"Erreur lors du scraping ou de la sauvegarde des fichiers CSV: {str(e)}"

def index():
    return render_template('index.html')
